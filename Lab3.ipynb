{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "E80liBeQg5W6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdb8IOwLmYZO",
        "outputId": "499ad9e5-86e2-4cfe-936e-edbe932a9983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipython-autotime\n",
            "  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipython-autotime) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython->ipython-autotime)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipython-autotime) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\n",
            "Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\n",
            "Using cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: jedi, ipython-autotime\n",
            "Successfully installed ipython-autotime-0.3.2 jedi-0.19.1\n",
            "time: 542 Âµs (started: 2024-08-24 04:18:18 +00:00)\n"
          ]
        }
      ],
      "source": [
        "!pip install ipython-autotime\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Variable"
      ],
      "metadata": {
        "id": "zjwDwVNlegM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[0,1],[2,6],[3,8]]) #x1, x2\n",
        "y = np.array([1,1,4])\n",
        "\n",
        "x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "print(x_b.shape)\n",
        "y = y.reshape(-1,1)\n",
        "print(y.shape)\n",
        "\n",
        "alpha = 0.01 # learning rate\n",
        "\n",
        "def cost_function(theta, x, y, N):\n",
        "  y_hat = x.dot(theta)\n",
        "  c = (1/(2*N))*np.sum((y_hat-y)**2)\n",
        "  return c"
      ],
      "metadata": {
        "id": "CFYVwAZLee3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 1.Batch Gradient descent (Multiple linear regression)"
      ],
      "metadata": {
        "id": "WQiP94_Zm4vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function"
      ],
      "metadata": {
        "id": "s1m3ywd-gu7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(alpha, x, y, ep=0.001, max_iter=10000):\n",
        "  converged = False\n",
        "  iter = 0\n",
        "  N = x.shape[0] # number of samples\n",
        "  print(\"Num of data = \",N)\n",
        "\n",
        "  # initial theta\n",
        "  theta =  np.random.random((x.shape[1],1))\n",
        "  print(theta)\n",
        "  print(\"Init theta.shape = \",theta.shape)\n",
        "\n",
        "  # total error, J(theta)\n",
        "  J = cost_function(theta, x, y, N)\n",
        "  print(\"First J = \",J)\n",
        "\n",
        "  # Iterate Loop\n",
        "  while not converged:\n",
        "\n",
        "    y_hat = x.dot(theta)\n",
        "    diff = y_hat - y\n",
        "    grad = x.T.dot(diff)\n",
        "\n",
        "    theta = theta - alpha * (1/N) * (grad)\n",
        "\n",
        "    assert theta.shape == (3,1) #This line makes sure that the shape of theta is still be the same.\n",
        "\n",
        "    #\n",
        "\n",
        "    J2 = cost_function(theta, x, y, N)\n",
        "\n",
        "    if abs(J-J2) <= ep:\n",
        "        print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
        "        converged = True\n",
        "\n",
        "    J = J2   # update error s\n",
        "    iter += 1  # update iter\n",
        "\n",
        "    if iter == max_iter:\n",
        "        print('       Max iterations exceeded!')\n",
        "        converged = True\n",
        "\n",
        "  #print(\"End converged iter = \",iter)\n",
        "  return theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCbpcQrDneVD",
        "outputId": "b14e6a0e-ef51-4631-a710-efb4207daf64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.91 ms (started: 2024-08-24 08:23:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "f3yUj7pigzfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  print(\"Batch Gradient descent\")\n",
        "  #x = np.array([[0,2,3],[1,6,8]]).T\n",
        "  x = np.array([[0,1],[2,6],[3,8]]) #x1, x2\n",
        "  y = np.array([1,1,4])\n",
        "\n",
        "  x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "\n",
        "  print(x_b.shape)\n",
        "  y = y.reshape(-1,1)\n",
        "  print(y.shape)\n",
        "\n",
        "  alpha = 0.01 # learning rate\n",
        "  #Training process\n",
        "  theta = gradient_descent(alpha, x_b, y, ep=0.000000000001, max_iter=1000000)\n",
        "  print (\"Theta = \", theta)\n",
        "\n",
        "  #predict trainned x\n",
        "  xtest = np.array([[4,9]])\n",
        "  xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "  y_p = xtest_b.dot(theta)\n",
        "  print(\"y predict = \",y_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOwjvVtunkjO",
        "outputId": "59aeec00-6e05-437a-b763-bc9d609cfad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Gradient descent\n",
            "(3, 3)\n",
            "(3, 1)\n",
            "Num of data =  3\n",
            "[[0.4477498 ]\n",
            " [0.51996791]\n",
            " [0.76027015]]\n",
            "Init theta.shape =  (3, 1)\n",
            "First J =  7.044225650135942\n",
            "       Converged, iterations:  277818 / 1000000\n",
            "Theta =  [[ 6.99874934]\n",
            " [14.9972509 ]\n",
            " [-5.99883427]]\n",
            "y predict =  [[12.99824447]]\n",
            "time: 4.41 s (started: 2024-08-24 08:23:21 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Stochastic GD"
      ],
      "metadata": {
        "id": "Y-X-XZAsnuDz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function"
      ],
      "metadata": {
        "id": "kGyjI7Guf6S-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sgd(alpha, x, y, ep=0.001, max_iter=10000):\n",
        "  converged = False\n",
        "  iter = 0\n",
        "  N = 1 # number of samples\n",
        "  print(\"Num of data = \",N)\n",
        "\n",
        "  # initial theta\n",
        "  theta =  np.random.random((x.shape[1],1))\n",
        "  print(\"Init theta.shape = \",theta.shape)\n",
        "\n",
        "  # total error, J(theta)\n",
        "  J = cost_function(theta, x, y, N)\n",
        "  print(\"First J = \",J)\n",
        "\n",
        "  # Iterate Loop\n",
        "  while not converged:\n",
        "\n",
        "    y_hat = x.dot(theta)\n",
        "    diff = y_hat - y\n",
        "    grad = x.T.dot(diff)\n",
        "\n",
        "    theta = theta - alpha * (grad)\n",
        "\n",
        "    assert theta.shape == (3,1) #This line makes sure that the shape of theta is still be the same.\n",
        "\n",
        "    # error\n",
        "    J2 = cost_function(theta, x, y, N)\n",
        "    if abs(J-J2) <= ep:\n",
        "        print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
        "        converged = True\n",
        "\n",
        "    J = J2   # update error s\n",
        "    gamma = 0.1\n",
        "    iter += 1  # update iter\n",
        "    # alpha = alpha/(1 + alpha*gamma*iter)\n",
        "\n",
        "    if iter == max_iter:\n",
        "        print('       Max iterations exceeded!')\n",
        "        converged = True\n",
        "\n",
        "  #print(\"End converged iter = \",iter)\n",
        "  return theta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyeUXO5un0_-",
        "outputId": "d07a0839-07cc-4022-a162-557ad1318de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.63 ms (started: 2024-08-24 08:12:56 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "2caHpqCIf-RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  print(\"Stochastic Gradient Decent\")\n",
        "  x = np.array([[0,1],[2,6],[3,8]]) #x1, x2\n",
        "  y = np.array([1,1,4])\n",
        "\n",
        "  x_b = np.c_[np.ones((x.shape[0],1)),x]\n",
        "  print(x_b.shape)\n",
        "  y = y.reshape(-1,1)\n",
        "  print(y.shape)\n",
        "\n",
        "  alpha = 0.01 # learning rate\n",
        "  #Training process\n",
        "  theta = sgd(alpha, x_b, y, ep=0.000000000001, max_iter=1000000)\n",
        "  print (\"Theta = \", theta)\n",
        "\n",
        "  #predict trainned x\n",
        "  xtest = np.array([[4,9]])\n",
        "  xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "  y_p = xtest_b.dot(theta)\n",
        "  print(\"y predict = \",y_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oeZUL9CsOSP",
        "outputId": "5e8810c2-c727-437a-faa0-a95499931726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stochastic Gradient Decent\n",
            "(3, 3)\n",
            "(3, 1)\n",
            "Num of data =  1\n",
            "Init theta.shape =  (3, 1)\n",
            "First J =  4.645812622712811\n",
            "       Converged, iterations:  104191 / 1000000\n",
            "Theta =  [[ 6.99958314]\n",
            " [14.99908369]\n",
            " [-5.99961145]]\n",
            "y predict =  [[12.99941486]]\n",
            "time: 2.32 s (started: 2024-08-24 08:14:27 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Mini-batch GD"
      ],
      "metadata": {
        "id": "3eMa5A5Wn8tR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function"
      ],
      "metadata": {
        "id": "lGJwba36gb1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini(alpha, x, y, ep=0.001, max_iter=10000):\n",
        "  converged = False\n",
        "  iter = 0\n",
        "  N = 2 # number of samples\n",
        "  print(\"Num of data = \",N)\n",
        "\n",
        "  # initial theta\n",
        "  theta =  np.random.random((x.shape[1],1))\n",
        "  print(\"Init theta.shape = \",theta.shape)\n",
        "\n",
        "  # total error, J(theta)\n",
        "  J = cost_function(theta, x, y, N)\n",
        "  print(\"First J = \",J)\n",
        "\n",
        "  # Iterate Loop\n",
        "  while not converged:\n",
        "\n",
        "    y_hat = x.dot(theta)\n",
        "    diff = y_hat - y\n",
        "    grad = x.T.dot(diff)\n",
        "\n",
        "    theta = theta - alpha * (grad)\n",
        "\n",
        "    assert theta.shape == (3,1) #This line makes sure that the shape of theta is still be the same.\n",
        "\n",
        "    # error\n",
        "    J2 = cost_function(theta, x, y, N)\n",
        "    if abs(J-J2) <= ep:\n",
        "        print(\"       Converged, iterations: \", iter, \"/\", max_iter)\n",
        "        converged = True\n",
        "\n",
        "    J = J2   # update error s\n",
        "    gamma = 0.1\n",
        "    iter += 1  # update iter\n",
        "    # alpha = alpha/(1 + alpha*gamma*iter)\n",
        "\n",
        "    if iter == max_iter:\n",
        "        print('       Max iterations exceeded!')\n",
        "        converged = True\n",
        "\n",
        "  #print(\"End converged iter = \",iter)\n",
        "  return theta"
      ],
      "metadata": {
        "id": "KCAp3d8BoJyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92335cc-8b31-428b-a139-f7d5cac8116d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.9 ms (started: 2024-08-24 08:13:24 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result"
      ],
      "metadata": {
        "id": "dkRV-OxogeuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  print(\"Mini_batch Gradieneet Decent\")\n",
        "\n",
        "  #Training process\n",
        "  theta = mini(alpha, x_b, y, ep=0.000000000001, max_iter=1000000)\n",
        "  print (\"Theta = \", theta)\n",
        "\n",
        "  #predict trainned x\n",
        "  xtest = np.array([[4,9]])\n",
        "  xtest_b = np.c_[np.ones((xtest.shape[0],1)),xtest]\n",
        "  y_p = xtest_b.dot(theta)\n",
        "  print(\"y predict = \",y_p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaLLYW6zOyRc",
        "outputId": "fe63961c-1d7c-4392-d182-5aa805f75a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mini_batch Gradieneet Decent\n",
            "(3, 3)\n",
            "(3, 1)\n",
            "Num of data =  2\n",
            "Init theta.shape =  (3, 1)\n",
            "First J =  23.668903018853094\n",
            "       Converged, iterations:  100550 / 1000000\n",
            "Theta =  [[ 6.99941046]\n",
            " [14.99870413]\n",
            " [-5.9994505 ]]\n",
            "y predict =  [[12.99917248]]\n",
            "time: 1.96 s (started: 2024-08-24 08:14:44 +00:00)\n"
          ]
        }
      ]
    }
  ]
}